<html lang="en">
<head>
  <meta charset="utf-8">

  <title>Graph AI</title>
  <link href="https://fonts.googleapis.com/css?family=Lato|Open+Sans" rel="stylesheet">
<style>
body {
	font-family: 'Open Sans', sans-serif;
}
div#content {
	padding:			50px;
	margin-left:		auto;
	margin-right:		auto;
	width:	    		800px;
	min-height:			420px;
	background-color:	#ffffff;
	position:			relative;
}
h1 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
}
h2 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
}
h3 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
	margin-left:  20px;
}
h4 {
	margin-left:  20px;
}
a {
	text-decoration: none;
	color: #000;
	border-bottom: 1px solid #d4d4d4;
}
a:hover {
	color: 			#fb6300;
}
table {
	margin: 20px;
	margin-left: 40px;
	border:#ccc 1px solid;
}
table tr {
	text-align: center;
	padding-left:20px;
}
table td {
	padding: 8px 24px;
	border: 1px solid #e0e0e0;
}
p {
	margin-left: 40px;
}
</style>

</head>

<body>

<div id="content">

<h1 style="text-align:center"> Student Research Opportunities </h1>

<center>
<h4>
Contact: <a href="https://chenxuhao.github.io/">Xuhao Chen</a> <br/>
</h4>
</center>

<p>
  We are looking for grad & undergrad students to join our lab.
  Feel free to reach out if you are interested in machine learning systems, computer architecture, and/or high performance computing.

  Our projects have the potential to become MEng thesis work. We have <a href="https://www.eecs.mit.edu/home/eecs-alliance/6a/">6-A program</a> opportunities available.
  If you are interested, please send your CV to <a href="mailto:cxh@mit.edu">cxh@mit.edu</a> and fill in the
  <a href="https://docs.google.com/forms/d/e/1FAIpQLSfAwaRSct0V3gnxntv2CXUK8fum5PHSF3_ZZQlM1pgUO2MwfQ/viewform">recruiting form</a>.
</p>


<p>
  <strong>Research Summary</strong>: The AI revolution is transforming various industries and having a significant impact on society.
  However, AI is computationally expensive and hard to scale, which poses a great challenge in computer system design.
  Our lab is broadly interested in computer system architectures and high performance computing, particularly for scaling AI and ML computation.
</p>

<p>
  Top-tier system & HPC conferences <a href="https://www.usenix.org/conferences">[OSDI, SOSP]</a>,
<a href="https://www.asplos-conference.org/">[ASPLOS]</a>,
<a href="https://iscaconf.org/">[ISCA]</a>,
<a href="https://www.vldb.org/">[VLDB]</a>,
<a href="https://sigmod.org/">[SIGMOD]</a>
<a href="https://supercomputing.org/">[SC]</a>,
<a href="https://conf.researchr.org/home/ppopp-2024">[PPoPP]</a>.
</p>


<p>
  <strong>Interesting MLSys topics</strong>: Vector Similarity Search, Recommendation System, Retrieval-Augmented Generation, Graph Machine Learning, Graph Algorithms, GPU Acceleration, LLM Serving/Inference
</p>

<p>
Below are some ongoing research projects.
</p>

<h2> Scalable Vector Database [<a href="https://elx.mit.edu/experience/c67c0e7e-34ef-4de9-a95f-05e92e5c88d3">Elx Link</a>]</h2>

  <p>
    Recent advances in deep learning models map almost all types of data (e.g., images, videos, documents) into high-dimension vectors.
    Queries on high-dimensional vectors enable complex semantic-analysis that was previously difficult if not impossible,
    thus they become the cornerstone for many important online services like search, eCommerce, and recommendation systems.
  </p>
  <p>
    <img src="images/vector-database.png" alt="Vector Database" style="width:570px;height:300px;">
  </p>

  <p>
    In this project we aim to build a massive-scale <strong>Vector Database</strong> on the multi-CPU and multi-GPU platform.
    In a Vector Database, the major operation is to search the k closest vectors to a given query vector,
    known as k-Nearest-Neighbor (kNN) search. Due to massive data scale, Approximate Nearest-Neighbor (ANN) search is used in practice instead.
    One of the most promising ANN approaches is the graph-based approach, which first constructs a proximity graph on the dataset,
    connecting pairs of vectors that are close to each other, then performs a graph traversal on the proximity
    graph for each query to find the closest vectors to a query vector. In this project we will build a vector
    database using graph-based ANN search algorithm that supports billion-scale datasets.
  </p>

  <p>
    Qualifications:
    <ul>
      <li>Strong programming skills in C/C++/Python language</li>
      <li>Experience with design and analysis of algorithms, e.g., <a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.1220 (previously 6.046)</a></li>
      <li>Experience with performance engineering is a plus, e.g., <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT 6.1060 (previously 6.172)</a></li>
      <li><a href="http://gputeachingkit.hwu.crhc.illinois.edu/">GPU/CUDA programming</a> is a plus</li>
    </ul>
  </p>

  <p> <strong>References</strong> </p>
  <ol>
    <li>  <a href="https://arxiv.org/pdf/2408.02937">A Real-Time Adaptive Multi-Stream GPU System for Online Approximate Nearest Neighborhood Search</a> , CIKM 2024</li>
    <li>  <a href="https://arxiv.org/pdf/2305.04359.pdf">ParlayANN</a>, PPoPP 2024. </li>
    <li>  <a href="https://arxiv.org/pdf/2308.15136.pdf">CAGRA</a>, ICDE 2024 </li>
    <li>  <a href="https://dl.acm.org/doi/pdf/10.1145/3572848.3577527">iQAN</a>, PPoPP 2023.  </li>
    <li>  <a href="https://arxiv.org/pdf/1702.08734.pdf ">Billion-scale similarity search with GPUs</a>  </li>
    <li>  <a href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf">HNSW</a>  </li>
    <li>  <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf">DiskANN</a>
         [<a href="https://www.microsoft.com/en-us/research/video/research-talk-approximate-nearest-neighbor-search-systems-at-scale/">Video</a>]
         [<a href="https://cvpr.thecvf.com/media/cvpr-2023/Slides/18545_SzZdLZD.pdf">Slides</a>]
         [<a href="https://people.csail.mit.edu/jshun/6506-s24/lectures/lecture21-2.pdf">Slides2</a>] </li>
    <li>  <a href="https://github.com/intel/ScalableVectorSearch">intel/ScalableVectorSearch</a>  </li>
    <li>  <a href="https://github.com/IntelLabs/VectorSearchDatasets">IntelLabs/VectorSearchDatasets</a>  </li>
  </ol>

  <h2> Zero-Knowledge Proof [<a href="https://elx.mit.edu/experience/c67c0e7e-34ef-4de9-a95f-05e92e5c88d3">Elx Link</a>]</h2>

    <p>
      Zero-knowledge proof (ZKP) is a cryptographic method of proving the validity of a statement without revealing anything other than the validity of the statement itself.
      This “zero-knowledge” property is attractive for many privacy-preserving applications, such as blockchain and cryptocurrency systems.
      Despite its great potential, ZKP is notoriously compute intensive, which hampers its real-world adoption.
      Recent advances in cryptography, known as zk-SNARK, have brought ZKP closer to practical use.
      Although zk-SNARK enables fast verification of the proof, proof generation in ZKP is still quite expensive and slow.
    </p>

    <p>
      <img src="images/zkp.png" alt="Zero-Knowledge Proof" style="width:600px;height:300px;">
    </p>
    <p>
      In this project, we will explore ZKP acceleration by using algorithm innovations, software performance engineering, and parallel hardware like GPU, FPGA or even ASIC.
      We aim to investigate and implement efficient algorithms for accelerating elliptic curve computation.
      We will also explore acceleration opportunities for the major operations, e.g., finite field arithmetic, Multi-scalar Multiplication (MSM) and Number-theoretic transformations (NTT).
    </p>

    <p>
      Qualifications:
      <ul>
        <li>Strong programming skills in C/C++ language</li>
        <li>Experience with design and analysis of algorithms, e.g., <a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.1220 (previously 6.046)</a></li>
        <li>Experience with performance engineering is a plus, e.g., <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT 6.1060 (previously 6.172)</a></li>
        <li><a href="http://gputeachingkit.hwu.crhc.illinois.edu/">GPU/CUDA</a> and/or Rust/Web Assembly/Javascript programming is a plus</li>
      </ul>
    </p>

    <p> <strong>References</strong> </p>
    <ol>
      <li> <a href="https://dl.acm.org/doi/10.1145/3620666.3651364">Accelerating Multi-Scalar Multiplication for Efficient Zero Knowledge Proofs with Multi-GPU Systems</a>, ASPLOS 2024.  </li>
      <li> <a href="https://doi.org/10.1145/3575693.3575711">GZKP</a>. ASPLOS 2023.  </li>
      <li> <a href="https://eprint.iacr.org/2024/1246.pdf">MSMAC: Accelerating Multi-Scalar Multiplication for Zero-Knowledge Proof</a>. DAC 2024 </li>
      <li> <a href="https://ieeexplore.ieee.org/document/9499783">PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture</a>. ISCA 2021 </li>
      <li> <a href="https://dl.acm.org/doi/abs/10.1145/3626202.3637577">Hardcaml MSM: A High-Performance Split CPU-FPGA Multi-Scalar Multiplication Engine</a>. FPGA 2024 </li>
      <li> <a href="https://people.csail.mit.edu/devadas/pubs/micro24_nocap.pdf">Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design</a>. MICRO 2024 </li>
    </ol>

    <h2> Deep Recommendation System [<a href="https://elx.mit.edu/experience/c67c0e7e-34ef-4de9-a95f-05e92e5c88d3">Elx Link</a>]</h2>

    <p>
      Deep Learning Recommendation Models (DLRMs) are widely used across industry to provide personalized recommendations to users and consumers.
      Specifically, they are the backbone behind user engagement for industries such as ecommerce, entertainment, and social networks.
      DLRMs have two stages: (1) training, in which the model learns to minimize the difference between predicted and actual user interactions,
      and (2) inference, in which the model provides recommendations based on new data.
      Traditionally, GPUs have been the hardware component of choice for DLRM training because of the high computational demand.
      In contrast, CPUs have been widely used for DLRM inference due to tight latency requirements that restrict the batch size.
      An existing bottleneck in inference is the high computational and memory bandwidth, which contribute greatly to loads on data centers and computing clusters.
    </p>

    <p>
      <img src="images/DRS.png" alt="Deep Recommendation System" style="width:370px;height:300px;">
    </p>

      <p>
        In this project, we will focus on exploring GPU optimizations to the embedding stage of the DLRM inference pipeline, which has traditionally only utilized CPUs.
        Initially, we would like to explore CPU-GPU coupled schemes, for instance using GPUs as extra cache space (e.g. to store more embeddings or to memoize sparse feature computations),
        and multi-GPU cluster computation in order to further accelerate inference for more complex models.
        The goal is to coalesce existing inference frameworks, profile them, and implement novel ones to exhibit substantial speedup for DLRM inference on GPUs.
      </p>

      <p>
        Qualifications:
        <ul>
          <li>Strong programming skills in C/C++/Python language</li>
          <li>Experience with design and analysis of algorithms, e.g., <a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.1220 (previously 6.046)</a></li>
          <li>Experience with performance engineering is a plus, e.g., <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT 6.1060 (previously 6.172)</a></li>
          <li><a href="http://gputeachingkit.hwu.crhc.illinois.edu/">GPU/CUDA programming</a> is a plus</li>
        </ul>
      </p>

    <p> <strong>References</strong> </p>
    <ol>
      <li> <a href="https://doi.org/10.1145/3579371.3589112">Optimizing CPU Performance for Recommendation Systems At-Scale</a>. ISCA ’23.  </li>
      <li> <a href="https://doi.org/10.5281/zenodo.7699872">GRACE</a>, ASPLOS 2023. </li>
      <li> <a href="https://doi.org/10.1145/3575693.3575718">EVStore</a>. ASPLOS ‘23. </li>
    </ol>

<h2> Graph AI Systems [<a href="https://elx.mit.edu/experience/c67c0e7e-34ef-4de9-a95f-05e92e5c88d3">Elx Link</a>]</h2>

  <p>
      Deep Learning is good at capturing hidden patterns of <strong>Euclidean</strong> data (images, text, videos).
      But what about applications where data is generated from <strong>non-Euclidean</strong> domains,
      represented as graphs with complex relationships and interdependencies between objects?
      That’s where <a href= "https://aimagazine.com/machine-learning/what-graph-ai">Graph AI</a>
      or <a href= "https://arxiv.org/pdf/2105.00696.pdf">Graph ML</a> come in.
      Handling the complexity of graph data and graph algorithms requires innovations in every layer of the computer system, including both software and hardware.
  </p>
      <p>
        <img src="images/gai.png" alt="An overview of graph neural networks for anomaly detection in e-commerce" style="width:700px;height:180px;">
      </p>
  <p>
    In this project we will design and build efficient graph AI systems to support scalable graph AI computing.
    In particular, we will build software frameworks for Graph AI and ML, e.g., graph neural networks (GNN), graph pattern mining (GPM) and graph sampling,
    and hardware accelerators that further enhance system efficiency and scalability. </p>
  <p>
    Qualifications:
    <ul>
      <li>Strong programming skills in C/C++/Python language</li>
      <li>Experience with design and analysis of algorithms, e.g., <a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.1220 (previously 6.046)</a></li>
      <li>Basic understanding of computer architecture, e.g., <a href="https://6191.mit.edu/">MIT 6.1910 (previously 6.004)</a></li>
      <li>Experience with performance engineering is a plus, e.g., <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT 6.1060 (previously 6.172)</a></li>
      <li><a href="http://gputeachingkit.hwu.crhc.illinois.edu/">GPU/CUDA programming</a> is a plus</li>
      <li>Familiarity with deep learning frameworks is a plus, e.g., PyTorch</li>
    </ul>
  </p>

      <p> <strong>References</strong> </p>

<ol>
  <li> <a href="https://dl.acm.org/doi/pdf/10.14778/3681954.3681968">F^2CGT</a> VLDB 2024 </li>
  <li> <a href="https://dl.acm.org/doi/pdf/10.1145/3600006.3613168">gSampler</a>, SOSP 2023 [<a href="https://github.com/gsampler9/gSampler">Code</a>] </li>
  <li> <a href="https://github.com/chenxuhao/ReadingList/blob/master/sampling/NextDoor.pdf">NextDoor</a>, EuroSys 2021 [<a href="https://github.com/plasma-umass/nextdoor">Code</a>]  </li>
  <li> <a href="https://dl.acm.org/doi/pdf/10.1145/3511808.3557443">Scalable graph sampling on gpus with compressed graph</a>, CIKM 2022  </li>
</ol>

<h2> Graph AI for Financial Security  [<a href="https://elx.mit.edu/experience/e773c545-183f-4d4a-8c2b-06c56efb9e0f">Elx Link</a>] </h2>
  <p>
    The advent of cryptocurrency introduced by Bitcoin ignited an explosion of technological
    and entrepreneurial interest in payment processing.
    Dampening this excitement was Bitcoin’s bad reputation.
    Many criminals used Bitcoin’s pseudonymity to hide in plain sight,
    conducting ransomware attacks and operating dark marketplaces for the exchange of illegal goods and services.
  </p>
      <p>
        <img src="images/GNN-Fraud.webp" alt="An overview of graph neural networks for anomaly detection in e-commerce" style="width:500px;height:300px;">
      </p>
  <p>
    This project offers a golden opportunity to apply machine learning for financial forensics.
    The data of Bitcoin transactions naturally forms a financial transaction graph, in which we can apply graph machine learning and graph pattern mining techniques to automatically detect illegal activities.
    We will explore the identification and clustering of frequent subgraphs to uncover money laundering patterns, and conduct link predictions on the wallets (nodes) to unveil the malicious actor behind the scene.
  </p>
<p>
  Qualifications:

  <ul>
    <li>Strong programming skills in Python and C/C++ language</li>
    <li>Background and prior experience with design and analysis of algorithms (e.g., 6.046)</li>
    <li>Some familiarity with deep learning frameworks such as PyTorch</li>
  </ul>
</p>

    <p> <strong>References</strong> </p>

<ol>
  <li>  <a href="https://arxiv.org/pdf/2302.08043.pdf">GraphPrompt</a> </li>
  <li>  <a href="https://openreview.net/pdf?id=XLxhEjKNbXj">Glass</a> </li>
</ol>

<h2> AI/ML for Performance Engineering [<a href="https://elx.mit.edu/experience/e773c545-183f-4d4a-8c2b-06c56efb9e0f">Elx Link</a>] </h2>
  <p>
    Generative AI, such as Large Language Models (LLMs), has been successfully used to generate
    computer programs, a.k.a code generation. However, its model performance degrades substantially
    when asked to do code optimization a.k.a. software performance engineering (SPE), i.e., generate
    not just correct but fast code.
  </p>
      <p>
        <img src="images/fast-coder.png" alt="An overview of AI coder" style="width:700px;height:150px;">
      </p>
  <p>
    This project aims to leverage the capabilities of LLMs to revolutionize the area of automatic code optimization.
    We focus on transforming existing sequential code into high-performance, parallelized code, optimized for specific parallel hardware.
  </p>
<p>
  Qualifications:

  <ul>
    <li>Strong programming skills in Python and C/C++ language</li>
    <li>Background and prior experience with design and analysis of algorithms (e.g., 6.046)</li>
    <li>Familiarity with LLMs such as ChatGPT and Code Llama</li>
  </ul>
</p>

    <p> <strong>References</strong> </p>

<ol>
  <li> <a href="https://arxiv.org/pdf/2404.18864">Performance-Aligned LLMs for Generating Fast Code</a>  </li>
  <li> <a href="https://pie4perf.com/">Learning Performance Improving Code Edits</a>  </li>
  <li> <a href="https://arxiv.org/pdf/2401.12554.pdf">Can Large Language Models Write Parallel Code?</a>  </li>
  <li> <a href="https://arxiv.org/pdf/2402.09126.pdf">MPIrigen: MPI Code Generation through Domain-Specific Language Models</a>  </li>
  <li> <a href="https://arxiv.org/pdf/2402.02018.pdf">The Landscape and Challenges of HPC Research and LLMs</a>  </li>
</ol>



<h2> Efficient Robotics Computing [<a href="https://elx.mit.edu/experience/c67c0e7e-34ef-4de9-a95f-05e92e5c88d3">Elx Link</a>]</h2>

  <p>
    The advancement of robotics technology is rapidly changing the world we live in.
    With predictions of 20 million robots by 2030 and a market capitalization of US$210 billion by 2025, it is clear that robotics will play an increasingly important role in society.
    To become widespread, robots need to meet the demands of real-world environments,
    which necessitates them being autonomous and capable of performing complex artificial intelligence (AI) tasks in real-time.
  </p>
  <p>
    <img src="images/robot.jfif" alt="robot" style="width:570px;height:300px;">
  </p>

  <p>
    In this project we aim to build software and hardware systems for <strong>Robotics</strong>.
  </p>

  <p>
    Qualifications:
    <ul>
      <li>Strong programming skills in C/C++/Python language</li>
      <li>Experience with design and analysis of algorithms, e.g., <a href="https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/">MIT 6.1220 (previously 6.046)</a></li>
      <li>Experience with performance engineering is a plus, e.g., <a href="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/">MIT 6.1060 (previously 6.172)</a></li>
      <li>Parallel OpenMP or <a href="http://gputeachingkit.hwu.crhc.illinois.edu/">GPU/CUDA programming</a> is a plus</li>
    </ul>
  </p>

  <p> <strong>References</strong> </p>
  <ol>
  <li>
    Phillip B Gibbons (CMU)'s lab.
    <a href="https://github.com/cmu-roboarch/tartan">Tartan: Microarchitecting a Robotic Processor</a>,
    <a href="https://www.iscaconf.org/isca2024/program/">ISCA 2024</a>
  </li>
  <li>
    Tor Aamodt (UBC)'s Lab.
    <a href="https://people.ece.ubc.ca/~aamodt/publications/papers/shah.isca2024.pdf">Collision Prediction for Robotics Accelerators</a>,
    <a href="https://www.iscaconf.org/isca2024/program/">ISCA 2024</a>
  </li>
  <li>
    Tor Aamodt (UBC)'s Lab.
    <a href="https://people.ece.ubc.ca/~aamodt/publications/papers/shah.isca2023.pdf">Energy-Efficient Realtime Motion Planning</a>. ISCA 2023.
  </li>
  <li>
    Phillip B Gibbons (CMU)'s lab.
    <a href="https://www.pdl.cmu.edu/PDL-FTP/associated/bakshalipour-Agents-of-Autonomy.pdf">Agents of Autonomy: A Systematic Study of Robotics on Modern Hardware</a>. SIGMETRICS 2023.
  </li>
  <li>
    Sabrina M. Neuman, Vijay Janapa Reddi (Harvard)'s Lab.
    <a href="https://dl.acm.org/doi/pdf/10.1145/3579371.3589104">RoboShape</a>. ISCA 2023.
  </li>
  <li>    <a href="https://www.kavrakilab.org/publications.html">  Lydia Kavraki (Rice)'s Lab</a>  </li>
    Sophia Shao (Berkeley)'s lab.
    <a href="https://dl.acm.org/doi/abs/10.1145/3579371.3589099">RoSÉ</a>
  <li>
    <a href="https://ieeexplore.ieee.org/abstract/document/9439435">A Survey of FPGA-Based Robotic Computing</a>
  </li>
  <li>
    Phillip B Gibbons (CMU)'s lab.
    <a href="https://dl.acm.org/doi/abs/10.1145/3470496.3527383">RACOD</a>
    ISCA 2022.
  </li>
</ol>

</div>

</body>
</html>
