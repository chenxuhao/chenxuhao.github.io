<html lang="en">
<head>
  <meta charset="utf-8">

  <title>GPU Computing</title>
  <link href="https://fonts.googleapis.com/css?family=Lato|Open+Sans" rel="stylesheet">
<style>
body {
	font-family: 'Open Sans', sans-serif;
}
div#content {
	padding:			50px;
	margin-left:		auto;
	margin-right:		auto;
	width:	    		800px;
	min-height:			420px;
	background-color:	#ffffff;
	position:			relative;
}
h1 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
}
h2 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
}
h3 {
	color:			#80bd01;
	font-family: 'Lato', sans-serif;
	margin-left:  20px;
}
h4 {
	margin-left:  20px;
}
a {
	text-decoration: none;
	color: #000;
	border-bottom: 1px solid #d4d4d4;
}
a:hover {
	color: 			#fb6300;
}
table {
	margin: 20px;
	margin-left: 40px;
	border:#ccc 1px solid;
}
table tr {
	text-align: center;
	padding-left:20px;
}
table td {
	padding: 8px 24px;
	border: 1px solid #e0e0e0;
}
p {
	margin-left: 40px;
}
</style>

</head>

<body>
  
<div id="content">

<h2 style="text-align:center"> TBD </h2>
<h1 style="text-align:center"> GPU Computing </h1>

<center>
<h4> Location, Time <br/>
Instructor: <a href="https://chenxuhao.github.io/">Xuhao Chen</a> <br/>
Office Hours: Time, Location </h4>
</center>

<h2> Course Description </h2>

<p>This course is an introduction to parallel computing using graphics processing units (GPUs). We will be focussing on CUDA programming, but the concepts taught will apply to other GPU frameworks as well. The course will start by covering CUDA syntax extensions and the CUDA runtime API, then move on to more advanced topics such as bandwidth optimization, memory access performance, and floating point considerations. We will learn about common parallel computing patterns such as scans and reductions, and study use cases for GPU acceleration such as matrix multiplication and convolution.</p>

<h3> Prerequisites </h3>

<p>As CUDA is an extension of the C language, students taking this course should be familiar with C programming.</p>

<p>Prior knowledge of computer architecture concepts such as data locality will be useful but not required.</p>

<h3> Grading </h3>

<p>Grades for this course will be based on a series of 3-5 programming assignments designed to allow students to apply GPU programming skills taught in the lectures.</p>

<h3> Textbook (Optional) </h3>

<p> <strong>Programming Massively Parallel Processors, Third Edition: A Hands-on Approach</strong> <br/> David B. Kirk and Wen-mei W. Hwu. </p> 

<p> The Second Edition is online available <a href="https://safari.ethz.ch/architecture/fall2019/lib/exe/fetch.php?media=2013_programming_massively_parallel_processors_a_hands-on_approach_2nd.pdf">here</a> </p>

<h3> Computing Resources </h3>

<p> For the programming assignments, students will need access to a computer with a <a href="https://developer.nvidia.com/cuda-gpus">CUDA-compatible GPU</a>. 
I can help arrange access to a remote CUDA-capable machine for students without local access. </p>

<p> <a href="https://developer.nvidia.com/teaching-kits">The NVIDIA Deep Learning Institute (DLI) Teaching Kit Program</a> </p>

<p> <a href="http://webgpu.com/">WebGPU.com A System for Online GPU Development</a> </p>

<p> <a href="https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about">Fundamentals of Accelerated Computing with CUDA Python</a> </p>

<p> <a href="http://impact.crhc.illinois.edu/shared/PR/SC16_PMPP_Educators_Sessions_FINAL.pdf">Teach GPU Accelerating Computing: Hands-on with NVIDIA Teaching Kit for Educators</a> </p>

<h2> Schedule and Slides (subject to change)</h2>

<table cellspacing="0">
<tr> <td> 3 / 27 </td> <td> <a href="Lecture01-intro.pdf">Course Introduction</a> </td> <td> <a href="https://cse.usf.edu/~haozheng/ref/gpu_computing.pdf">Paper reading</a> </td> </tr>
<tr> <td> 3 / 29 </td> <td> <a href="Lecture02-cuda-c.pdf">Intro to CUDA C</a> </td> <td> <a href="assignments/axpy.tar.gz">axpy</a> </td> </tr>
<tr> <td> 4 / 03 </td> <td> <a href="Lecture03-model.pdf">CUDA parallelism model</a> </td> <td> <a href="https://dl.acm.org/doi/pdf/10.1145/1365490.1365500">Paper reading</a> </td> </tr>
<tr> <td> 4 / 05 </td> <td> <a href="Lecture04-locality.pdf">Memory and data locality</a> <br/> <a href="Lecture05-thread.pdf">Thread execution efficiency</a> </td> <td> <a href="assignments/tiled-sgemm.tar.gz">TiledMatrixMultiplication</a> </td> </tr>
<tr> <td> 4 / 10 </td> <td> <a href="Lecture06-memory.pdf">Memory performance</a> <br/> <a href="Lecture07-stencil.pdf">Stencil pattern</a> </td> <td> <a href="https://uweb.engr.arizona.edu/~ece569a/Readings/GPU_Papers/1.ComputingExperiences.pdf">Paper reading</a> </td> </tr>
<tr> <td> 4 / 12 </td> <td> <a href="Lecture08-scan.pdf">Prefix sum pattern</a> </td> <td> <a href="https://developer.download.nvidia.com/compute/cuda/2_2/sdk/website/projects/scan/doc/scan.pdf">Paper reading</a> </td> </tr>
<tr> <td> 4 / 17 </td> <td> <a href="Lecture09-histogram.pdf">Histogram pattern</a> </td> <td> TiledMatrixMultiplication due </td> </tr>
<tr> <td> 4 / 19 </td> <td> <a href="Lecture10-spmv.pdf">Sparse matrix pattern</a> </td> <td> <a href="https://www.nvidia.com/docs/IO/77944/sc09-spmv-throughput.pdf">Paper reading</a> </td> </tr>
<tr> <td> 4 / 24 </td> <td> <a href="Lecture11-reduction.pdf">Reduction pattern</a> </td> <td> <a href="assignment2">Assignment 2</a> </td>  </tr>
<tr> <td> 4 / 26 </td> <td> <a href="Lecture12-bfs.pdf">Graph traversal pattern</a> </td> <td> <a href="https://mgarland.org/files/papers/gpubfs.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 01 </td> <td> <a href="Lecture13-interface.pdf">Advanced host / device interface</a> <br/> <a href="Lecture14-stream.pdf">Streams, events, and concurrency</a> </td> <td> <a href="https://mgarland.org/files/papers/gpusort-ipdps09.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 03 </td> <td> <a href="Lecture15-dynamic-parallelism.pdf">Dynamic parallelism / recursion</a> </td> <td> <a href="https://static.twoday.net/romanov/files/Exzerpt2.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 08 </td> <td> <a href="Lecture16-float.pdf">Floating point considerations</a> <br /> <a href="Lecture17-intrinsic.pdf">Intrinsic Functions</a> </td> <td> Assignment 2 due <br /> <a href="finalProject"/>Final project</a> </td> </tr>
<tr> <td> 5 / 10 </td> <td> <a href="Lecture18-warp-shuffle.pdf">In-warp shuffles</a> </td> <td> <a href="http://users.ece.utexas.edu/~ljohn/teaching/382m-15/reading/lee.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 15 </td> <td> <a href="Lecture19-multi-gpu.pdf">Multi-GPU programming</a> </td> <td> Final project proposal due </td> </tr>
<tr> <td> 5 / 17 </td> <td> <a href="Lecture20-library.pdf">Using CUDA Libraries</a> </td> <td> <a href="http://dlsys.cs.washington.edu/schedule">cuDNN</a> </td> </tr>
<tr> <td> 5 / 22 </td> <td> <a href="Lecture21-opencl-openacc.pdf">OpenCL / OpenACC</a> </td> <td> <a href="https://engineering.purdue.edu/paramnt/publications/PPOPP09-LME.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 24 </td> <td> <a href="Lecture22-tensor.pdf">Deep Learning and Tensor Core </td> <td> <a href="https://arxiv.org/pdf/1811.09736.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 29 </td> <td> <a href="Lecture23-graph.pdf">Graph Processing with GPU</a> </td> <td> <a href="https://readingxtra.github.io/docs/gpu-graph/WangPPoPP2016.pdf">Paper reading</a> </td> </tr>
<tr> <td> 5 / 31 </td> <td> <a href="https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/">Ray Tracing</a> </td> <td> Final project due </td> </tr>
</table>

</div>

</body>
</html>
